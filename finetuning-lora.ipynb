{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hcd9u-xB54ml",
    "outputId": "3e98f2e4-b880-41f1-8381-664f3d1ca3ef"
   },
   "outputs": [],
   "source": [
    "#login to hugging face\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iusKhHmm51wF"
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.41.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ok6az_-C8-uK",
    "outputId": "55a4b843-1806-4dc6-d585-cb97f7f8aedf"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade accelerate peft bitsandbytes transformers trl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wE1cuWPz8eoq",
    "outputId": "e7c47b65-e644-4b86-a6f2-9a6175dae4df"
   },
   "outputs": [],
   "source": [
    "!pip install tokenizers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM_Tn9J0IabA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHxhhx9wIatm"
   },
   "source": [
    "# **Text preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj-o2kXHIinp",
    "outputId": "fbba41b4-f5e0-42ea-9904-bbfca9062a6e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#load dataset\n",
    "df=pd.read_csv(\"/content/train.csv\")\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQDSKEBTIiqi",
    "outputId": "b7b14f75-1ba9-4c4e-cdf0-a0002dcd61b2"
   },
   "outputs": [],
   "source": [
    "print(\"\\nðŸ” Null values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oR_1ZaaOzD4o"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['DDX SNOMED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqQf1eKUIi31",
    "outputId": "35525eb0-5f80-4783-cb32-f5a508472a46"
   },
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Unique values per column:\")\n",
    "for col in df.columns:\n",
    "    unique_vals = df[col].nunique()\n",
    "    print(f\"\\nðŸ”¹ Column: {col}\")\n",
    "    print(f\"   Unique Count: {unique_vals}\")\n",
    "\n",
    "    # Print actual unique values, limiting to first 20 if too many\n",
    "    unique_items = df[col].unique()\n",
    "    if len(unique_items) > 20:\n",
    "        print(f\"   Sample Values (20 of {len(unique_items)}): {unique_items[:20]}\")\n",
    "    else:\n",
    "        print(f\"   Values: {unique_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lOFsPCevIi8q",
    "outputId": "988af7a2-3670-4c21-ab80-971623341e59"
   },
   "outputs": [],
   "source": [
    "# 3. Plot count of unique values (for categorical columns)\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() < 50]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    df[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f\"Value Counts for '{col}'\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDhwpdOWnU-J",
    "outputId": "cfeeff69-65c7-47c3-9a4e-38df380e1516"
   },
   "outputs": [],
   "source": [
    "def convert_to_chat_template_format(df):\n",
    "    conversations = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        convo = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a clinical decision support assistant. \"\n",
    "                        f\"Context:\\n\"\n",
    "                        f\"- location: {row['County']}, Kenya\\n\"\n",
    "                        f\"- Facility: {row['Health level']}\\n\"\n",
    "                        f\"- Nursing Competency: {row['Nursing Competency']}\\n\"\n",
    "                        f\"- Clinical Panel: {row['Clinical Panel']}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": row[\"Prompt\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"Clinician\"]},\n",
    "            ]\n",
    "        }\n",
    "        conversations.append(convo)\n",
    "\n",
    "    return conversations\n",
    "chat_data = convert_to_chat_template_format(df)\n",
    "chat_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHXSeJ4dnVgG"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "# Get the max sequence length from the tokenizer\n",
    "max_length_tokenizer = tokenizer.model_max_length\n",
    "\n",
    "# Add special tokens (if necessary)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Apply the chat template formatting\n",
    "def apply_chat_template(convo):\n",
    "    formatted_messages = []\n",
    "    for message in convo[\"messages\"]:\n",
    "        formatted_message = f\"{message['role']}: {message['content']}\"\n",
    "        formatted_messages.append(formatted_message)\n",
    "    return \"\\n\".join(formatted_messages)\n",
    "\n",
    "# Function to tokenize conversations using the chat template\n",
    "def tokenize_conversations_with_chat_template(conversations, tokenizer, max_length=max_length_tokenizer ):\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for convo in conversations:\n",
    "        formatted_convo = apply_chat_template(convo)\n",
    "        \n",
    "        # Tokenize the formatted conversation\n",
    "        tokenized_convo = tokenizer(\n",
    "            formatted_convo, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        # Shift the labels for causal language modeling (next token prediction)\n",
    "        labels = tokenized_convo['input_ids'].clone()  # Clone to keep a copy for labels\n",
    "        labels[:, :-1] = tokenized_convo['input_ids'][:, 1:].clone()  # Shift labels by 1\n",
    "        labels[:, -1] = tokenizer.pad_token_id  # Set the last token as padding in the labels\n",
    "        \n",
    "        # Append the tokenized conversation with labels\n",
    "        tokenized_data.append({\n",
    "            \"input_ids\": tokenized_convo['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "            \"attention_mask\": tokenized_convo['attention_mask'].squeeze(0),  # Remove batch dimension\n",
    "            \"labels\": labels.squeeze(0)  # Remove batch dimension\n",
    "        })\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Example conversion and tokenization\n",
    "tokenized_chat_data = tokenize_conversations_with_chat_template(chat_data, tokenizer)\n",
    "\n",
    "# Example output of the first tokenized conversation\n",
    "print(tokenized_chat_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7Htq_aUnVt2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2MITX6XnV5g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC4IyM_fnWCF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjsKGyeLIjH1"
   },
   "source": [
    "# **Finetuning LLM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRos1A8954pr"
   },
   "outputs": [],
   "source": [
    "# Import necessary packages for the fine-tuning process\n",
    "import os                          # Operating system functionalities\n",
    "import torch                       # PyTorch library for deep learning\n",
    "from datasets import load_dataset  # Loading datasets for training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,          # AutoModel for language modeling tasks\n",
    "    AutoTokenizer,                # AutoTokenizer for tokenization\n",
    "    BitsAndBytesConfig,           # Configuration for BitsAndBytes\n",
    "    HfArgumentParser,             # Argument parser for Hugging Face models\n",
    "    TrainingArguments,            # Training arguments for model training\n",
    "    pipeline,                     # Creating pipelines for model inference\n",
    "    logging,                      # Logging information during training\n",
    ")\n",
    "from peft import LoraConfig, PeftModel  # Packages for parameter-efficient fine-tuning (PEFT)\n",
    "from trl import SFTTrainer         # SFTTrainer for supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O38U8LV54tT"
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "#model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"finetuned_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRx39NdF542r"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6UObY3u545-"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "#per_device_eval_batch_size = 8\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnHNn_HM5482"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zm2pn9nE6Vvw"
   },
   "outputs": [],
   "source": [
    "# # Step 1 : Load dataset (you can process it here)\n",
    "# # The instruction dataset to use\n",
    "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# dataset = load_dataset(dataset_name, split=\"train\")\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3DajBeOChmq"
   },
   "outputs": [],
   "source": [
    "# def tokenize(example):\n",
    "#     return tokenizer(\n",
    "#         example[\"text\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=512,  # ðŸ‘ˆ set it here\n",
    "#     )\n",
    "\n",
    "# tokenized_dataset = formatted_text.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ozsU5CyClzp"
   },
   "outputs": [],
   "source": [
    "# tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QP1PikMS6V0M"
   },
   "outputs": [],
   "source": [
    "# Step 2 :Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Step 3 :Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "OTdehcmU6V6Z",
    "outputId": "68672281-7d9a-4059-ab38-71945997dc3d"
   },
   "outputs": [],
   "source": [
    "# Step 4 :Load base model\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model_base .config.use_cache = False\n",
    "model_base .config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbDA3UiS54yV"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 8\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAm7kYii6V8a"
   },
   "outputs": [],
   "source": [
    "# Step 6 :Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\"]#[\"q_proj\"],# \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNAhw0tMEWW4",
    "outputId": "def628dd-56d5-4e9b-fa8e-bebcef9f199e"
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBzGp3Sz6WBU"
   },
   "outputs": [],
   "source": [
    "# Step 7 :Set training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # Directory to save model checkpoints & logs\n",
    "    num_train_epochs=num_train_epochs,  # How many times to iterate over the full dataset\n",
    "    per_device_train_batch_size=per_device_train_batch_size,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # Simulates larger batch sizes by accumulating gradients\n",
    "    optim=optim,  # Optimizer (e.g., \"adamw_torch\")\n",
    "    save_steps=save_steps,  # How often to save a checkpoint\n",
    "    logging_steps=logging_steps,  # How often to log training info\n",
    "    learning_rate=learning_rate,  # Initial learning rate\n",
    "    weight_decay=weight_decay,  # L2 regularization\n",
    "    fp16=fp16,  # Use mixed precision (16-bit floating point)\n",
    "    bf16=bf16,  # Use bfloat16 (on compatible hardware, like A100s)\n",
    "    max_grad_norm=max_grad_norm,  # Gradient clipping to prevent exploding gradients\n",
    "    max_steps=max_steps,  # Max training steps (overrides `num_train_epochs` if set)\n",
    "    warmup_ratio=warmup_ratio,  # Portion of training used for learning rate warm-up\n",
    "    group_by_length=group_by_length,  # Whether to group sequences of similar lengths for efficient batching\n",
    "    lr_scheduler_type=lr_scheduler_type,  # Scheduler type (e.g. \"cosine\", \"linear\", etc.)\n",
    "    report_to=\"tensorboard\"  # Reporting backend (could be \"wandb\", \"tensorboard\", etc.)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182,
     "referenced_widgets": [
      "79ae96b5f7de4f38b347f24625b23452",
      "751a5eff6398461b82f41073baef8702",
      "1044c4c5495f476ba968a6e4201275a5",
      "c77be2cc83e34d3bb675373da5d05753",
      "465dc02ef34746ff9e2f9d605102dbce",
      "80628a7b53764730b147a4c14ccb332c",
      "91325c86cc4644dea798c1af1c69ce36",
      "9b3a26be54f14abcbf12edf0711402d0",
      "70c791f8f31b43a69823af9e05d7cf88",
      "5d16f5664c534b51855d6f7309ebb342",
      "de5dd722f8bc4157b941374caff48ebd",
      "808d1d04384c4a698360d136dbc9f89f",
      "95f970a0cf704579aa1f2cd36993ca33",
      "513d751fd8034a0a9648f6363961ca4a",
      "be0bf03fd5c84e98aa35f581c13249e5",
      "8ff97a0d7cae43f69cca97791e39f666",
      "6cb5023182704e7abca2cc5ed1a5fae2",
      "0f94c192bce54c6aa0a3eb0f1d08c300",
      "ed658653b3a340d7be9e4e973723f15d",
      "011857f78fc542e596a3d21f1bcd21c1",
      "aade875718fd4e23babb6b1878fd8973",
      "25d42d215de94b96923882a93b117196",
      "c7d60a829b354de8b6ee7998d26f8ce9",
      "501667c429de4313b9677fe1271807c6",
      "d6d9b428939c433fbf18f12f211cb482",
      "fa3654b4c14f46e9a60dad908a3fe999",
      "e892a9ce05cc4482847572ad1402d763",
      "4f07441899e549828dd3523b3b074232",
      "c58b6313aa0842438bdfa5fcdccaf99c",
      "5a0e2f352e3c4c7b9607c456bc2cc4c3",
      "a09375a467b942c8a663bdccbbe8d830",
      "bf350647947d413a8735eb3c110634ee",
      "260e781dd624470b9f840d3d95cb55d9",
      "32b36d82c63b4ddab34f8ae560a0bf70",
      "8fb01357a562470db494ade9ddb77a8b",
      "3d1454fd52a148a29e45f475d2bd2912",
      "ff31f01c90fe43ef9ba980f2c85870d9",
      "79267babc09f4a8fbc349d46fe8e837c",
      "40fc0f4813cf4f38aff1a8fa1890e79b",
      "6f4ae19b6251448385e0cae100e8613e",
      "9fe2329d550f466799f5f76e35967bde",
      "8f703ec6394447e3a426fc2f2402daa3",
      "d9c2ba6b5c3c4ebca9473a05629de101",
      "aa182f8f05ad4587bc6ead714cbe57f9"
     ]
    },
    "id": "S1Tc5vsg62y8",
    "outputId": "e5893b92-a972-48a2-8491-063f928188a5"
   },
   "outputs": [],
   "source": [
    "# Step 8 :Set supervised fine-tuning parameters\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "jCDjp1BP622W",
    "outputId": "180b4869-cd6f-431a-d618-f51249f2625f"
   },
   "outputs": [],
   "source": [
    "# Step 9 :Train model\n",
    "trainer.train()\n",
    "\n",
    "# Step 10 :Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "#save tokenizer\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0J3nA6D625s"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOyt4CSM629q"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel  # This is assuming you're using PEFT (such as LoRA or other PEFT methods)\n",
    "\n",
    "# Path to the base foundation model (e.g., GPT-2, T5, etc.)\n",
    "foundation_model_path = \"HuggingFaceTB/SmolLM2-135M-Instruct\"  # E.g., \"gpt2\", or the path to your base model directory\n",
    "\n",
    "\n",
    "# Path to the fine-tuned PEFT model (your checkpoint directory)\n",
    "peft_model_path = \"/kaggle/working/results/checkpoint-100\"  # Path to the PEFT model directory\n",
    "\n",
    "# Load the base foundation model (e.g., GPT-2)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(foundation_model_path)\n",
    "\n",
    "# Load the PEFT (fine-tuned) model\n",
    "fine_tuned_model = PeftModel.from_pretrained(foundation_model,\n",
    "                                             peft_model_path,\n",
    "                                             is_trainable=False)\n",
    "\n",
    "# Load the tokenizer (make sure the tokenizer is compatible with the model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(foundation_model_path)\n",
    "\n",
    "# Verify the loaded fine-tuned model\n",
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"i am a nurse with 18 years of experience in general nursing working in a sub county hospitals and nursing homes in uasin gishu county in kenya a 4 year old child presents to the emergency department with second degree burns on the forearm after accidentally touching a hot stove the child was playing in the kitchen when they reached out to touch the stove the burns cover about 5 of the total body surface area the child is alert and crying with redness blisters and swelling on the affected area the burns appear to be superficial to moderate in severity the child is in mild pain and there is no indication of airway or breathing distress no other injuries are noted questions 1 what is the immediate treatment protocol for second degree burns in paediatric patients 2 should any tetanus prophylaxis be considered in this case 3 what follow up care should be recommended for burn healing\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using the fine-tuned model with added parameters for diversity\n",
    "generated_text = fine_tuned_model.generate(\n",
    "    **inputs, \n",
    "    max_length=500, \n",
    "    num_return_sequences=1, \n",
    "    do_sample=True,       # Allow sampling for more diversity\n",
    "    temperature=1.0,      # Increase temperature for more creativity\n",
    "    top_p=0.9,            # Nucleus sampling for more diverse outputs\n",
    "    no_repeat_ngram_size=2  # Prevent repeated phrases\n",
    ")\n",
    "\n",
    "# Decode the generated tokens into readable text and skip the input part\n",
    "output = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "\n",
    "# Print only the generated text (not the input prompt)\n",
    "print(output[len(input_text):])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7248773,
     "sourceId": 11560849,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
